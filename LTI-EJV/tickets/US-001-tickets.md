# Descomposici√≥n T√©cnica: US-001 - Parseo Inteligente de CV con IA

**Proyecto:** LTI (Leading Talent Intelligence)
**Sprint:** Sprint 1
**Prioridad:** P1 (Alta) - MUST HAVE
**Estimaci√≥n Total:** L (2-3 semanas)
**Fecha de Creaci√≥n:** 2025-01-09

---

## üìã Resumen de la User Story Original

**Como** Recruiter
**Quiero** que el sistema extraiga autom√°ticamente informaci√≥n estructurada de los CVs que suben los candidatos (PDF, Word, LinkedIn)
**Para** ahorrar tiempo de entrada manual de datos y garantizar que todos los perfiles est√©n completos y normalizados desde el primer momento

### Criterios de Aceptaci√≥n Principales
- Soporte para PDF, DOCX, TXT, y URLs de LinkedIn
- Precisi√≥n >85% en extracci√≥n de datos cr√≠ticos
- Procesamiento <30 segundos para 95% de CVs
- Normalizaci√≥n autom√°tica de habilidades
- Detecci√≥n de duplicados
- Vista editable de datos extra√≠dos
- Fallback a revisi√≥n manual en casos de baja confianza

---

## üèóÔ∏è Arquitectura T√©cnica de la Soluci√≥n

### Componentes del Sistema

```mermaid
graph TB
    subgraph "Frontend - React 19.2"
        UI[CV Upload UI]
        Preview[Preview & Edit Component]
        Status[Upload Status Dashboard]
    end

    subgraph "API Gateway - Node.js + TypeScript"
        API[REST API Gateway]
        Auth[Authentication Middleware]
        Upload[File Upload Handler]
    end

    subgraph "Storage Layer"
        S3[AWS S3<br/>CV Storage]
        MySQL[(MySQL Database<br/>Structured Data)]
    end

    subgraph "Message Queue"
        Queue[RabbitMQ<br/>Parse Job Queue]
    end

    subgraph "CV Parser Service - Python + FastAPI"
        Parser[CV Parser Orchestrator]
        PDFParser[PDF Parser<br/>PyPDF2/pdfplumber]
        DOCXParser[DOCX Parser<br/>python-docx]
        LinkedInParser[LinkedIn Parser]
        NLPEngine[NLP Engine<br/>spaCy NER]
        Normalizer[Skills Normalizer]
        OCR[AWS Textract<br/>Fallback OCR]
    end

    subgraph "Support Services"
        Cache[Redis Cache<br/>Parsed Results]
        Logger[Logging Service]
    end

    UI -->|Upload CV| API
    API -->|Store File| S3
    API -->|Enqueue Job| Queue
    Queue -->|Process| Parser
    Parser -->|Extract Text| PDFParser
    Parser -->|Extract Text| DOCXParser
    Parser -->|Scrape Data| LinkedInParser
    PDFParser -->|Complex PDF| OCR
    Parser -->|Extract Entities| NLPEngine
    NLPEngine -->|Normalize| Normalizer
    Parser -->|Save Structured Data| MySQL
    Parser -->|Cache Result| Cache
    Parser -->|Notify| API
    API -->|Update UI| Preview

    style UI fill:#e1f5ff
    style API fill:#fff4e6
    style Parser fill:#f3e5f5
    style MySQL fill:#e8f5e9
    style Queue fill:#fce4ec
```

### Stack Tecnol√≥gico Detallado

| Componente | Tecnolog√≠a | Justificaci√≥n |
|------------|------------|---------------|
| **Frontend** | React 19.2 + TypeScript | UI moderna con type safety |
| **API Gateway** | Node.js 20 + Express + TypeScript | Unificaci√≥n del stack backend |
| **Parser Service** | Python 3.11 + FastAPI | Ecosistema NLP/ML maduro |
| **Database** | MySQL 8.0 | Almacenamiento relacional estructurado |
| **File Storage** | AWS S3 (o MinIO local) | Escalable, versionado de CVs |
| **Message Queue** | RabbitMQ | Procesamiento as√≠ncrono confiable |
| **Cache** | Redis 7 | Evitar reprocesamiento de CVs |
| **NLP Engine** | spaCy 3.7 (es_core_news_lg/en_core_web_lg) | NER pre-entrenado, extensible |
| **OCR** | AWS Textract API | PDFs escaneados/complejos |
| **Container** | Docker + Docker Compose | Desarrollo y despliegue consistente |

---

## üé´ Tickets T√©cnicos Descompuestos

### TICKET US-001-01: Database Schema - Candidate & Skills Tables

**Tipo**: Technical
**Componente**: Database
**Asignado a**: Backend Dev
**Estimaci√≥n**: 2 d√≠as

#### Descripci√≥n
Dise√±ar e implementar el schema de base de datos MySQL para almacenar candidatos, skills, y CVs parseados. Debe soportar datos estructurados y semi-estructurados (JSON para parsed_resume).

#### Tareas T√©cnicas
- [ ] Dise√±ar schema completo con relaciones (CANDIDATE, CANDIDATE_SKILL, SKILLS, PARSED_CV_CACHE)
- [ ] Crear tabla `candidates` con campos: id (UUID), email (UNIQUE), full_name, phone, location, current_title, current_company, years_experience, linkedin_url, resume_url (S3), parsed_resume (JSON), created_at, updated_at
- [ ] Crear tabla `skills` con diccionario maestro: id, skill_name (UNIQUE), category, synonyms (JSON), created_at
- [ ] Crear tabla `candidate_skills`: id, candidate_id (FK), skill_id (FK), proficiency_level (enum), years_experience, is_verified, source (parsed/manual)
- [ ] Crear tabla `parsed_cv_cache`: id, file_hash (UNIQUE), parsed_data (JSON), confidence_score, created_at, expires_at
- [ ] Crear √≠ndices: idx_candidates_email, idx_candidates_created_at, idx_skills_name, idx_candidate_skills_composite
- [ ] Implementar migrations con herramienta (Knex.js, TypeORM migrations, o similar)
- [ ] Seed de diccionario inicial de skills t√©cnicos (m√≠nimo 200 skills comunes en tech)

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] Todas las tablas creadas con constraints (FK, UNIQUE, NOT NULL apropiados)
- [ ] Migrations ejecutan correctamente en MySQL 8.0
- [ ] Rollback de migrations funciona sin errores
- [ ] √çndices creados mejoran queries en >50% (medido con EXPLAIN)
- [ ] JSON schema v√°lido para campos parsed_resume y synonyms
- [ ] Seed data carga correctamente 200+ skills

#### Dependencias
- Depende de: Ninguna (primer ticket)
- Bloquea a: US-001-02, US-001-03, US-001-06

#### Definici√≥n T√©cnica de Hecho
- [ ] Schema documentado en archivo `schema.sql` con comentarios
- [ ] Migrations versionadas (ej: `001_create_candidates_table.ts`)
- [ ] Tests de integraci√≥n para constraints (FK violations, unique violations)
- [ ] Script de seed data en `seeds/001_initial_skills.ts`
- [ ] ER Diagram exportado (usando herramienta como dbdiagram.io)
- [ ] README con instrucciones de setup de DB
- [ ] Code review aprobado

#### Notas de Implementaci√≥n
- **Tipo de datos para JSON**: Usar tipo `JSON` nativo de MySQL 8.0 (no TEXT)
- **UUIDs**: Usar `CHAR(36)` o `BINARY(16)` con funciones UUID_TO_BIN/BIN_TO_UUID
- **Particionamiento**: Considerar partition por `created_at` si se anticipa alto volumen (>1M registros)
- **Soft deletes**: Agregar campo `deleted_at` para candidatos (no borrado f√≠sico)
- **Skill synonyms format**: JSON array: `["React", "ReactJS", "React.js", "react"]`

#### Artefactos
- [ ] `backend/database/migrations/001_create_candidates.ts`
- [ ] `backend/database/migrations/002_create_skills.ts`
- [ ] `backend/database/migrations/003_create_candidate_skills.ts`
- [ ] `backend/database/migrations/004_create_parsed_cv_cache.ts`
- [ ] `backend/database/seeds/001_initial_skills.ts`
- [ ] `backend/database/schema.sql` (consolidated schema)
- [ ] `docs/database/er-diagram.png`

---

### TICKET US-001-02: Backend API - File Upload Endpoints

**Tipo**: Feature
**Componente**: Backend
**Asignado a**: Backend Dev
**Estimaci√≥n**: 3 d√≠as

#### Descripci√≥n
Implementar endpoints REST en Node.js/Express para manejar upload de CVs (PDF, DOCX, TXT), validaci√≥n de archivos, y storage en S3. Incluye generaci√≥n de URLs firmadas y metadata.

#### Tareas T√©cnicas
- [ ] Configurar multer middleware para manejo de multipart/form-data (l√≠mite 10MB)
- [ ] Implementar POST `/api/v1/candidates/upload-cv` con validaci√≥n de tipo de archivo
- [ ] Validar extensiones permitidas: .pdf, .docx, .doc, .txt (whitelist MIME types)
- [ ] Generar hash SHA-256 del archivo para deduplicaci√≥n
- [ ] Configurar AWS SDK v3 para S3 (o MinIO para desarrollo local)
- [ ] Upload archivo a S3 bucket con estructura: `cvs/{year}/{month}/{uuid}-{hash}.{ext}`
- [ ] Generar presigned URL con expiraci√≥n de 24h para acceso temporal
- [ ] Crear registro en tabla `candidates` con status "pending_parse"
- [ ] Publicar mensaje a RabbitMQ queue `cv-parse-jobs` con metadata
- [ ] Implementar POST `/api/v1/candidates/upload-linkedin` para URLs de LinkedIn
- [ ] Validar formato de URL de LinkedIn (regex)
- [ ] Responder con `201 Created` y job_id para tracking de progreso

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] Archivos >10MB rechazados con error 413 Payload Too Large
- [ ] Tipos de archivo no permitidos rechazados con error 400 Bad Request
- [ ] Upload exitoso retorna job_id y status URL (`/api/v1/jobs/{job_id}`)
- [ ] Duplicados detectados por hash retornan datos cached (evitar re-parsing)
- [ ] S3 upload falla gracefully con retry (3 intentos, exponential backoff)
- [ ] Rate limiting configurado: max 10 uploads/minuto por usuario
- [ ] Validaci√≥n de autenticaci√≥n con JWT middleware
- [ ] Logs estructurados con correlation IDs (winston + morgan)

#### Dependencias
- Depende de: US-001-01 (Database Schema)
- Bloquea a: US-001-04, US-001-05

#### Definici√≥n T√©cnica de Hecho
- [ ] C√≥digo implementado en `backend/src/routes/candidates.routes.ts`
- [ ] Controller en `backend/src/controllers/candidates.controller.ts`
- [ ] Service layer en `backend/src/services/upload.service.ts`
- [ ] Tests unitarios para validaciones (>80% cobertura)
- [ ] Tests de integraci√≥n con S3 mock (usando localstack o aws-sdk-mock)
- [ ] Tests E2E con Supertest simulando upload real
- [ ] Swagger/OpenAPI documentation actualizada
- [ ] Variables de entorno documentadas en `.env.example`
- [ ] Error handling con c√≥digos HTTP apropiados
- [ ] Security audit: validaci√≥n de MIME type real (no solo extensi√≥n)

#### Notas de Implementaci√≥n
- **Multipart parsing**: Usar `multer` con storage en memoria (no disco), luego stream a S3
- **S3 Bucket naming**: `lti-cvs-{env}` (lti-cvs-dev, lti-cvs-prod)
- **File hash**: Calcular con `crypto.createHash('sha256').update(fileBuffer).digest('hex')`
- **Deduplicaci√≥n**: Buscar en `parsed_cv_cache` por file_hash antes de parsear
- **LinkedIn URL validation**: Regex: `^https://www\.linkedin\.com/in/[\w-]+/?$`
- **RabbitMQ message format**: `{jobId, candidateId, fileUrl, fileType, uploadedBy, timestamp}`
- **Presigned URLs**: Usar `@aws-sdk/s3-request-presigner` con expiraci√≥n configurable

#### Artefactos
- [ ] `backend/src/routes/candidates.routes.ts`
- [ ] `backend/src/controllers/candidates.controller.ts`
- [ ] `backend/src/services/upload.service.ts`
- [ ] `backend/src/middleware/file-validation.middleware.ts`
- [ ] `backend/src/config/s3.config.ts`
- [ ] `backend/src/utils/file-hash.util.ts`
- [ ] `backend/tests/integration/upload.test.ts`
- [ ] `backend/docs/api/upload-endpoints.md`

---

### TICKET US-001-03: Backend API - Job Status & Results Endpoints

**Tipo**: Feature
**Componente**: Backend
**Asignado a**: Backend Dev
**Estimaci√≥n**: 2 d√≠as

#### Descripci√≥n
Implementar endpoints para consultar estado de jobs de parseo y recuperar resultados parseados. Incluye endpoints para editar datos extra√≠dos antes de confirmar.

#### Tareas T√©cnicas
- [ ] Implementar GET `/api/v1/jobs/{jobId}` para estado de job (pending, processing, completed, failed)
- [ ] Implementar GET `/api/v1/candidates/{candidateId}/parsed-data` para datos extra√≠dos
- [ ] Implementar PUT `/api/v1/candidates/{candidateId}/parsed-data` para ediciones manuales
- [ ] Implementar POST `/api/v1/candidates/{candidateId}/confirm` para confirmar y persistir datos
- [ ] Implementar GET `/api/v1/candidates/{candidateId}/duplicates` para candidatos similares
- [ ] Crear tabla `parse_jobs`: id, candidate_id, status, progress (%), error_message, started_at, completed_at
- [ ] Implementar l√≥gica de detecci√≥n de duplicados (email exact match, nombre+tel√©fono fuzzy match)
- [ ] Soporte para paginaci√≥n en listado de candidatos duplicados
- [ ] WebSocket endpoint (opcional) para real-time updates de progreso

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] GET /jobs/{jobId} retorna status actualizado en <200ms
- [ ] Datos parseados incluyen confidence score por campo (0-100)
- [ ] Ediciones manuales marcan campos como "manually_edited: true" en JSON
- [ ] Confirmaci√≥n dispara evento "candidate_confirmed" para US-005 (Screening)
- [ ] Duplicados detectados con algoritmo fuzzy (Levenshtein distance <3 para nombre)
- [ ] Response time <500ms para detecci√≥n de duplicados en base de 10k candidatos
- [ ] Validaci√≥n de ownership: solo el usuario que cre√≥ el candidato puede editarlo

#### Dependencias
- Depende de: US-001-01 (Database Schema), US-001-02 (Upload API)
- Bloquea a: US-001-07 (Frontend UI)

#### Definici√≥n T√©cnica de Hecho
- [ ] Endpoints implementados con OpenAPI spec completa
- [ ] Tests unitarios para l√≥gica de duplicados (>80% cobertura)
- [ ] Tests de integraci√≥n para flujo completo: upload ‚Üí poll status ‚Üí get data ‚Üí edit ‚Üí confirm
- [ ] Rate limiting: max 100 requests/minuto por usuario para polling
- [ ] Caching de parsed data con Redis (TTL 1 hora)
- [ ] Logs de auditor√≠a para ediciones manuales
- [ ] Documentation actualizada con ejemplos de payloads
- [ ] Code review aprobado

#### Notas de Implementaci√≥n
- **Job status enum**: `PENDING | PROCESSING | COMPLETED | FAILED | CANCELLED`
- **Progress tracking**: Actualizar progress en intervalos: 0% (queued), 25% (parsing), 50% (NLP), 75% (normalization), 100% (done)
- **Fuzzy matching**: Usar biblioteca `fuzzball` o `leven` para Levenshtein distance
- **WebSocket (opcional)**: Usar `socket.io` con rooms por jobId: `socket.join(`job:${jobId}`)`
- **Confidence score calculation**: Agregar scores por campo: `{full_name: 95, email: 100, skills: 78}`
- **Manual edits tracking**: `{field: "skills", original_value: [...], edited_value: [...], edited_by: userId, edited_at: timestamp}`

#### Artefactos
- [ ] `backend/src/routes/jobs.routes.ts`
- [ ] `backend/src/routes/candidates-data.routes.ts`
- [ ] `backend/src/services/job-status.service.ts`
- [ ] `backend/src/services/duplicate-detection.service.ts`
- [ ] `backend/src/utils/fuzzy-match.util.ts`
- [ ] `backend/tests/unit/duplicate-detection.test.ts`
- [ ] `backend/docs/api/job-status-endpoints.md`

---

### TICKET US-001-04: Message Queue - RabbitMQ Setup & Consumers

**Tipo**: Technical
**Componente**: Backend/Infrastructure
**Asignado a**: Backend Dev
**Estimaci√≥n**: 2 d√≠as

#### Descripci√≥n
Configurar RabbitMQ para procesamiento as√≠ncrono de jobs de parseo. Implementar producer (desde API) y consumer (que delega a Parser Service). Incluye dead letter queues y retry logic.

#### Tareas T√©cnicas
- [ ] Setup RabbitMQ server en Docker Compose (imagen oficial `rabbitmq:3.12-management`)
- [ ] Crear exchange `cv-processing-exchange` (tipo: direct)
- [ ] Crear queue `cv-parse-jobs` con binding a exchange
- [ ] Crear dead letter queue `cv-parse-jobs-dlq` para mensajes fallidos
- [ ] Implementar producer en Node.js usando `amqplib` (publicar mensaje en upload exitoso)
- [ ] Implementar consumer en Node.js que escucha `cv-parse-jobs`
- [ ] Consumer hace HTTP request al Parser Service (Python) con file URL
- [ ] Configurar retry logic: 3 reintentos con exponential backoff (1min, 5min, 15min)
- [ ] Mensajes con 3 fallos van a DLQ y marcan job como "failed"
- [ ] Implementar monitoring de queue depth (alarma si >100 mensajes pendientes)
- [ ] Configurar prefetch count = 5 (procesar max 5 mensajes concurrentes)

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] Mensajes persisten en disco (durable: true) para evitar p√©rdida en crash
- [ ] Consumer ACK manual (autoAck: false) solo despu√©s de parsing exitoso
- [ ] Dead letter queue captura mensajes fallidos con metadata de error
- [ ] Retry logic implementado con headers `x-retry-count`
- [ ] RabbitMQ Management UI accesible en http://localhost:15672
- [ ] Healthcheck endpoint `/api/v1/health/rabbitmq` retorna estado de conexi√≥n
- [ ] Graceful shutdown: consumer procesa mensajes en flight antes de terminar

#### Dependencias
- Depende de: US-001-02 (Upload API - producer), US-001-05 (Parser Service - consumer target)
- Bloquea a: Ninguno (habilita procesamiento as√≠ncrono)

#### Definici√≥n T√©cnica de Hecho
- [ ] RabbitMQ configurado en `docker-compose.yml` con volumes persistentes
- [ ] Producer implementado en `backend/src/services/queue/producer.service.ts`
- [ ] Consumer implementado en `backend/src/services/queue/consumer.service.ts`
- [ ] Tests de integraci√≥n: publicar mensaje ‚Üí consumer procesa ‚Üí ACK
- [ ] Tests de retry logic con mensaje fallido simulado
- [ ] Monitoring script que consulta queue depth cada 1 minuto
- [ ] Documentation de arquitectura de colas
- [ ] Variables de entorno para configuraci√≥n (RABBITMQ_URL, QUEUE_NAME, etc.)
- [ ] Code review aprobado

#### Notas de Implementaci√≥n
- **Connection string**: `amqp://user:password@localhost:5672`
- **Message payload format**:
  ```json
  {
    "jobId": "uuid-v4",
    "candidateId": "uuid-v4",
    "fileUrl": "https://s3.../cv.pdf",
    "fileType": "application/pdf",
    "uploadedBy": "userId",
    "timestamp": "ISO-8601",
    "attemptNumber": 1
  }
  ```
- **Retry headers**: `x-retry-count`, `x-first-death-queue`, `x-first-death-reason`
- **Exponential backoff**: Usar `message TTL` con valores crecientes: 60000ms, 300000ms, 900000ms
- **Graceful shutdown**: Escuchar `SIGTERM` y llamar `channel.close()` despu√©s de `channel.waitForConfirms()`
- **Connection recovery**: Auto-reconnect con `heartbeat: 60` seconds

#### Artefactos
- [ ] `docker-compose.yml` (RabbitMQ service)
- [ ] `backend/src/services/queue/producer.service.ts`
- [ ] `backend/src/services/queue/consumer.service.ts`
- [ ] `backend/src/config/rabbitmq.config.ts`
- [ ] `backend/tests/integration/queue.test.ts`
- [ ] `docs/architecture/message-queue.md`
- [ ] `scripts/rabbitmq-monitor.sh` (monitoring script)

---

### TICKET US-001-05: Parser Service - Python FastAPI Core

**Tipo**: Feature
**Componente**: AI/Parser Service
**Asignado a**: Backend Dev (Python)
**Estimaci√≥n**: 5 d√≠as

#### Descripci√≥n
Desarrollar servicio de parseo en Python + FastAPI que extrae datos estructurados de CVs (PDF, DOCX, TXT). Incluye extracci√≥n de texto, NER con spaCy, normalizaci√≥n de skills, y c√°lculo de confidence scores.

#### Tareas T√©cnicas
- [ ] Crear proyecto FastAPI con estructura modular (routers, services, models)
- [ ] Implementar POST `/parse` endpoint que recibe URL de archivo S3
- [ ] Download archivo desde S3 usando boto3
- [ ] **PDF Parsing**: Implementar con PyPDF2 para PDFs simples, pdfplumber para tablas
- [ ] **DOCX Parsing**: Implementar con python-docx
- [ ] **TXT Parsing**: Lectura directa con encoding detection (chardet)
- [ ] **NLP Extraction**: Usar spaCy NER para extraer:
  - Nombre completo (PERSON entities)
  - Email (regex: `r'[\w\.-]+@[\w\.-]+\.\w+'`)
  - Tel√©fono (regex con variaciones internacionales)
  - Ubicaci√≥n (GPE entities)
  - Skills t√©cnicos (custom entity recognizer)
  - Educaci√≥n (custom patterns: universidad, degree, a√±o)
  - Experiencia laboral (custom patterns: empresa, cargo, fechas)
- [ ] Implementar normalizaci√≥n de skills usando diccionario de MySQL (consulta via HTTP a API backend)
- [ ] Calcular confidence score por campo (basado en n√∫mero de matches y posici√≥n en CV)
- [ ] Guardar resultado parseado en MySQL (via API backend POST /candidates/{id}/parsed-data)
- [ ] Cachear resultado en Redis por file_hash (TTL 7 d√≠as)
- [ ] Manejo de errores: OCR fallback para PDFs escaneados (AWS Textract API)

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] Parseo completo de CV simple (<5 p√°ginas) en <10 segundos
- [ ] Precisi√≥n >85% en campos cr√≠ticos (nombre, email, skills) medida en dataset de 100 CVs
- [ ] Manejo de PDFs multi-columna sin p√©rdida de contexto
- [ ] Extracci√≥n de m√≠nimo 10 skills por CV (si existen en texto)
- [ ] Response JSON incluye confidence scores por campo
- [ ] OCR fallback se activa autom√°ticamente para PDFs con <50% texto extra√≠ble
- [ ] Service responde con 503 Service Unavailable si spaCy model no cargado

#### Dependencias
- Depende de: US-001-01 (Database), US-001-02 (S3 URLs)
- Bloquea a: US-001-04 (Consumer depende de este servicio), US-001-06 (NLP Training)

#### Definici√≥n T√©cnica de Hecho
- [ ] FastAPI service corriendo en contenedor Docker
- [ ] Endpoint `/parse` documentado con OpenAPI (auto-generado por FastAPI)
- [ ] Tests unitarios para cada parser (PDF, DOCX, TXT) con >80% cobertura
- [ ] Tests de integraci√≥n con archivos reales de diferentes formatos
- [ ] Performance testing: 100 CVs procesados en <15 minutos (paralelizado)
- [ ] spaCy models pre-descargados en imagen Docker (no download en runtime)
- [ ] Logging estructurado con correlation IDs (loguru)
- [ ] Health check endpoint `/health` con status de dependencies
- [ ] Dockerfile optimizado con multi-stage build (<500MB imagen final)
- [ ] Code review aprobado

#### Notas de Implementaci√≥n
- **spaCy models**: Descargar `es_core_news_lg` y `en_core_web_lg` en Dockerfile
- **Custom NER training**: Opcional para skills, ver US-001-06 si es necesario
- **Skills extraction fallback**: Si NER falla, usar keyword matching con lista de 500+ skills comunes
- **Confidence score formula**:
  ```python
  confidence = min(100, (
      (num_matches / expected_matches) * 50 +
      (field_found_in_first_half ? 30 : 10) +
      (exact_match ? 20 : fuzzy_match ? 10 : 0)
  ))
  ```
- **OCR activation**: Si `len(extracted_text) < len(pdf_pages) * 100` caracteres
- **Textract cost**: ~$1.50 per 1000 pages, usar solo cuando necesario
- **Response format**:
  ```json
  {
    "candidateId": "uuid",
    "parsedData": {
      "full_name": {"value": "John Doe", "confidence": 95},
      "email": {"value": "john@example.com", "confidence": 100},
      "skills": [
        {"name": "Python", "normalized": "Python", "confidence": 90},
        {"name": "React.js", "normalized": "React", "confidence": 85}
      ],
      "experience": [...],
      "education": [...]
    },
    "overallConfidence": 87,
    "processingTime": 8.3
  }
  ```

#### Artefactos
- [ ] `parser-service/app/main.py` (FastAPI app)
- [ ] `parser-service/app/routers/parse.py`
- [ ] `parser-service/app/services/pdf_parser.py`
- [ ] `parser-service/app/services/docx_parser.py`
- [ ] `parser-service/app/services/nlp_extractor.py`
- [ ] `parser-service/app/services/skills_normalizer.py`
- [ ] `parser-service/app/utils/confidence_calculator.py`
- [ ] `parser-service/Dockerfile`
- [ ] `parser-service/requirements.txt`
- [ ] `parser-service/tests/test_parsers.py`
- [ ] `parser-service/tests/fixtures/sample_cvs/` (PDFs, DOCXs de prueba)
- [ ] `parser-service/docs/api-spec.yaml`

---

### TICKET US-001-06: NLP Training - Custom spaCy Entity Recognizer

**Tipo**: Technical
**Componente**: AI/ML
**Asignado a**: ML Engineer / Backend Dev (Python)
**Estimaci√≥n**: 3 d√≠as

#### Descripci√≥n
Entrenar modelo custom de spaCy NER para mejorar detecci√≥n de skills t√©cnicos, t√≠tulos de trabajo, y empresas en CVs. Fine-tuning sobre modelos pre-entrenados.

#### Tareas T√©cnicas
- [ ] Recolectar dataset de entrenamiento: 100+ CVs anonimizados etiquetados manualmente
- [ ] Etiquetar entidades custom: SKILL, JOB_TITLE, COMPANY, DEGREE, CERTIFICATION
- [ ] Usar herramienta de anotaci√≥n (Label Studio, Prodigy, o Doccano)
- [ ] Exportar anotaciones en formato spaCy training (JSON)
- [ ] Implementar training script con spaCy CLI o API de entrenamiento
- [ ] Fine-tune `es_core_news_lg` y `en_core_web_lg` con datos custom
- [ ] Evaluar modelo en test set: medir precision, recall, F1-score por entidad
- [ ] Target: F1 >0.80 para SKILL, >0.75 para JOB_TITLE
- [ ] Versionar modelos con MLflow o DVC
- [ ] Integrar modelo entrenado en Parser Service (reemplazar base model)

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] Dataset de training: m√≠nimo 100 CVs, 1000+ entidades etiquetadas
- [ ] Modelo entrenado mejora F1 en >10% vs modelo base para SKILL entities
- [ ] Training pipeline automatizado con script reproducible
- [ ] Modelo entrenado <200MB (para no inflar imagen Docker)
- [ ] Evaluaci√≥n en test set (20% de datos) documentada con m√©tricas
- [ ] Modelo versionado con tag (ej: `skill-ner-v1.0`)

#### Dependencias
- Depende de: US-001-05 (Parser Service base)
- Bloquea a: Ninguno (mejora incremental)

#### Definici√≥n T√©cnica de Hecho
- [ ] Dataset de training en `parser-service/training/data/annotated_cvs.json`
- [ ] Training script en `parser-service/training/train_ner.py`
- [ ] Modelo entrenado en `parser-service/models/skill_ner_v1/`
- [ ] Evaluation report en `parser-service/training/evaluation_report.md`
- [ ] Tests con modelo nuevo vs modelo base (comparaci√≥n de m√©tricas)
- [ ] Documentation de proceso de training y actualizaci√≥n de modelo
- [ ] Modelo integrado en Parser Service y desplegado
- [ ] Code review aprobado

#### Notas de Implementaci√≥n
- **Annotation tool**: Usar Prodigy (si hay licencia) o Label Studio (open-source)
- **Training config**:
  ```python
  config = {
      "model": "es_core_news_lg",  # base model
      "new_entity_labels": ["SKILL", "JOB_TITLE", "COMPANY", "DEGREE"],
      "iterations": 30,
      "dropout": 0.2,
      "batch_size": 32
  }
  ```
- **Data augmentation**: Considerar t√©cnicas de augmentation para skills (sin√≥nimos, abreviaciones)
- **Evaluation metrics**: Usar `spacy evaluate` o implementar custom con sklearn.metrics
- **Versionado**: Nombrar modelos con semantic versioning: `skill_ner_v{major}.{minor}.{patch}`
- **Deployment**: Reemplazar modelo en Dockerfile con `COPY models/skill_ner_v1 /app/models/`

#### Artefactos
- [ ] `parser-service/training/data/annotated_cvs.json`
- [ ] `parser-service/training/train_ner.py`
- [ ] `parser-service/training/evaluate_ner.py`
- [ ] `parser-service/models/skill_ner_v1/` (modelo entrenado)
- [ ] `parser-service/training/evaluation_report.md`
- [ ] `parser-service/docs/model-training-guide.md`

---

### TICKET US-001-07: Frontend UI - CV Upload Component

**Tipo**: Feature
**Componente**: Frontend
**Asignado a**: Frontend Dev
**Estimaci√≥n**: 4 d√≠as

#### Descripci√≥n
Desarrollar componente React para upload de CVs con drag-and-drop, validaci√≥n de archivos, barra de progreso, y preview de datos parseados. Incluye UI para edici√≥n manual de campos extra√≠dos.

#### Tareas T√©cnicas
- [ ] Crear componente `CVUploadZone` con drag-and-drop usando `react-dropzone`
- [ ] Validaci√≥n client-side: tipo de archivo, tama√±o <10MB
- [ ] Mostrar preview de archivo seleccionado con metadata (nombre, tama√±o, tipo)
- [ ] Implementar upload con `axios` o `fetch` a `/api/v1/candidates/upload-cv`
- [ ] Mostrar barra de progreso durante upload (usando `onUploadProgress`)
- [ ] Polling de estado de job con intervalo de 2 segundos (GET `/api/v1/jobs/{jobId}`)
- [ ] Mostrar spinner con mensaje "Parsing CV... 45%" durante procesamiento
- [ ] Componente `ParsedDataPreview` para mostrar datos extra√≠dos en cards/sections
- [ ] Highlight de campos con baja confianza (<70%) en color naranja
- [ ] Componente `EditableField` para editar manualmente cualquier campo
- [ ] Bot√≥n "Confirm & Save" que llama POST `/api/v1/candidates/{id}/confirm`
- [ ] Manejo de errores: mostrar mensaje si parsing falla con opci√≥n de "Upload another"
- [ ] Detecci√≥n de duplicados: modal de advertencia si se encuentran candidatos similares

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] Drag-and-drop funciona en Chrome, Firefox, Safari, Edge
- [ ] Validaciones client-side previenen upload de archivos inv√°lidos
- [ ] Progress bar actualiza suavemente (no saltos bruscos)
- [ ] Polling se detiene autom√°ticamente cuando job status = "completed" o "failed"
- [ ] UI responsive: funciona en desktop (1920x1080) y tablet (768x1024)
- [ ] Loading states apropiados en todos los async calls
- [ ] Accesibilidad: navegaci√≥n con teclado, aria-labels en controles
- [ ] Ediciones manuales se guardan localmente antes de confirmar (prevent data loss)

#### Dependencias
- Depende de: US-001-02 (Upload API), US-001-03 (Status API)
- Bloquea a: US-001-09 (E2E Tests)

#### Definici√≥n T√©cnica de Hecho
- [ ] Componente implementado en `frontend/src/components/CVUpload/`
- [ ] Storybook stories para componente (aislado, diferentes estados)
- [ ] Tests unitarios con React Testing Library (>80% cobertura)
- [ ] Tests de integraci√≥n con MSW (Mock Service Worker) para APIs
- [ ] Accesibilidad auditada con axe-core (0 violations)
- [ ] Performance: Time to Interactive <3 segundos en 3G
- [ ] Code review aprobado
- [ ] Documentation en Storybook con ejemplos de uso

#### Notas de Implementaci√≥n
- **Libraries**:
  - `react-dropzone` para drag-and-drop
  - `axios` con interceptors para progress
  - `react-query` para polling y cache de status
  - `react-hook-form` para edici√≥n de campos
  - UI library: Material-UI o Chakra UI (consistente con design system)
- **Polling strategy**: Usar `setInterval` que se limpia en cleanup, o `react-query` con `refetchInterval`
- **Optimistic UI**: Mostrar datos parseados inmediatamente cuando confidence >90%, editable de todos modos
- **Error messages**: Mensajes user-friendly, no exponer detalles t√©cnicos
- **Duplicate modal**: Mostrar lista de duplicados con botones "Use existing" o "Create new anyway"
- **File preview**: Mostrar √≠cono seg√∫n tipo de archivo (PDF, Word, etc.)

#### Artefactos
- [ ] `frontend/src/components/CVUpload/CVUploadZone.tsx`
- [ ] `frontend/src/components/CVUpload/ParsedDataPreview.tsx`
- [ ] `frontend/src/components/CVUpload/EditableField.tsx`
- [ ] `frontend/src/components/CVUpload/DuplicateWarningModal.tsx`
- [ ] `frontend/src/hooks/useCVUpload.ts` (custom hook para l√≥gica)
- [ ] `frontend/src/hooks/useJobPolling.ts`
- [ ] `frontend/src/api/candidates.api.ts` (API client)
- [ ] `frontend/src/components/CVUpload/CVUpload.stories.tsx`
- [ ] `frontend/src/components/CVUpload/__tests__/CVUpload.test.tsx`
- [ ] `frontend/docs/components/cv-upload.md`

---

### TICKET US-001-08: Integration - LinkedIn Profile Scraper

**Tipo**: Feature
**Componente**: Parser Service
**Asignado a**: Backend Dev (Python)
**Estimaci√≥n**: 3 d√≠as

#### Descripci√≥n
Implementar scraper para extraer datos de perfiles p√∫blicos de LinkedIn. Usar API oficial si est√° disponible, fallback a scraping autorizado. Incluye rate limiting y manejo de anti-bot.

#### Tareas T√©cnicas
- [ ] Investigar LinkedIn API oficial (requiere partnership) - prioridad 1
- [ ] Si API disponible: implementar OAuth flow y calls a Profile API
- [ ] Si API NO disponible: implementar scraper con `playwright` o `selenium`
- [ ] Scraper debe extraer: nombre, headline, ubicaci√≥n, empresa actual, educaci√≥n, skills, experiencia
- [ ] Implementar rate limiting: max 10 profiles/minuto (evitar ban)
- [ ] Manejo de CAPTCHA: detecci√≥n y fallback a manual review
- [ ] Guardar HTML source en S3 para auditor√≠a (opcional, considerar privacidad)
- [ ] Parsear HTML extra√≠do con BeautifulSoup o lxml
- [ ] Normalizar datos extra√≠dos al mismo formato que parser de PDF
- [ ] Implementar caching de profiles por URL (evitar re-scraping)

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] Extracci√≥n exitosa de >90% de campos en perfiles p√∫blicos est√°ndar
- [ ] Detecci√≥n de perfiles privados/no accesibles con mensaje claro
- [ ] Rate limiting implementado con queue interna (no exceder l√≠mites)
- [ ] Scraper resiliente a cambios de estructura HTML (usar selectores robustos)
- [ ] Timeout de 30 segundos por profile (evitar hang indefinido)
- [ ] Logs de cada scraping attempt (success, failed, reason)

#### Dependencias
- Depende de: US-001-02 (API para recibir URL), US-001-05 (Parser Service base)
- Bloquea a: Ninguno (feature complementaria)

#### Definici√≥n T√©cnica de Hecho
- [ ] LinkedIn scraper implementado en `parser-service/app/services/linkedin_scraper.py`
- [ ] Tests unitarios con HTML fixtures (>80% cobertura)
- [ ] Tests de integraci√≥n con perfiles p√∫blicos reales (3-5 ejemplos)
- [ ] Documentation de limitaciones y requirements (API key si necesario)
- [ ] Error handling para casos edge (profile deleted, private, etc.)
- [ ] Compliance check: scraping autorizado, respeta robots.txt
- [ ] Code review aprobado

#### Notas de Implementaci√≥n
- **API vs Scraping**: Preferir API oficial si est√° disponible (m√°s estable, legal)
- **Scraping libraries**:
  - `playwright` (recomendado, headless browser con anti-detection)
  - `selenium` (alternativa)
  - `beautifulsoup4` + `lxml` para parsing
- **Selectors**: Usar data attributes o classes estables, evitar IDs generados
- **Rate limiting**: Implementar token bucket con `asyncio.Semaphore` o `ratelimit` library
- **CAPTCHA handling**: Si detectado, marcar job como "manual_review_required"
- **Legal compliance**: Revisar Terms of Service de LinkedIn, scraping de datos p√∫blicos generalmente OK pero gris area
- **User-Agent**: Usar User-Agent realista: `Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36`
- **Cookies**: Considerar login con cuenta de servicio para acceder a m√°s datos (requiere mantenimiento de sesi√≥n)

#### Artefactos
- [ ] `parser-service/app/services/linkedin_scraper.py`
- [ ] `parser-service/app/utils/linkedin_parser.py`
- [ ] `parser-service/tests/test_linkedin_scraper.py`
- [ ] `parser-service/tests/fixtures/linkedin_profiles.html`
- [ ] `parser-service/docs/linkedin-integration.md`

---

### TICKET US-001-09: Testing - E2E & Performance Tests

**Tipo**: Testing
**Componente**: Full Stack
**Asignado a**: QA / FullStack Dev
**Estimaci√≥n**: 3 d√≠as

#### Descripci√≥n
Implementar suite completa de tests end-to-end que cubren flujo completo de upload de CV hasta confirmaci√≥n de datos. Incluye performance tests para validar SLA de <30 segundos.

#### Tareas T√©cnicas
- [ ] Setup Playwright o Cypress para E2E tests
- [ ] Implementar test: "Upload PDF CV ‚Üí Poll status ‚Üí View parsed data ‚Üí Edit field ‚Üí Confirm"
- [ ] Implementar test: "Upload DOCX CV ‚Üí Verify all fields extracted ‚Üí Check duplicates"
- [ ] Implementar test: "Upload invalid file ‚Üí Verify error message"
- [ ] Implementar test: "Upload LinkedIn URL ‚Üí Verify scraping ‚Üí Confirm data"
- [ ] Performance test: Upload 10 CVs concurrentemente, verificar <30s cada uno
- [ ] Performance test: Upload 100 CVs, medir throughput (CVs/minuto)
- [ ] Load test con Artillery o k6: simular 50 usuarios concurrentes uploading
- [ ] Implementar smoke tests para validar deployment: "Can upload and parse basic CV"
- [ ] Integrar E2E tests en CI/CD pipeline (GitHub Actions o similar)

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] E2E tests cubren happy path y 3+ error scenarios
- [ ] Tests pasan consistentemente (>95% success rate en 10 runs)
- [ ] Performance tests validan SLA: 95% de CVs parseados en <30 segundos
- [ ] Load tests identifican l√≠mite de capacidad (ej: max 200 concurrent uploads)
- [ ] Tests ejecutan en <10 minutos totales (paralelizados)
- [ ] Screenshots/videos capturados en fallos para debugging
- [ ] Reportes de tests generados en formato HTML (Playwright report, Mochawesome, etc.)

#### Dependencias
- Depende de: US-001-07 (Frontend UI), US-001-05 (Parser Service), US-001-02 (Backend API)
- Bloquea a: Ninguno (completa testing suite)

#### Definici√≥n T√©cnica de Hecho
- [ ] E2E tests implementados en `tests/e2e/cv-upload.spec.ts`
- [ ] Performance tests en `tests/performance/cv-parsing-load.js` (k6 o Artillery)
- [ ] Tests integrados en CI pipeline (ejecutan en cada PR)
- [ ] Test data fixtures en `tests/fixtures/sample_cvs/`
- [ ] Documentation de c√≥mo ejecutar tests localmente
- [ ] Threshold configurado en CI: tests deben pasar para merge
- [ ] Code coverage report generado (combine frontend + backend)
- [ ] Code review aprobado

#### Notas de Implementaci√≥n
- **E2E tool selection**: Playwright (recomendado, multi-browser, video recording) vs Cypress (mejor DX pero solo Chromium)
- **Test data**: Usar CVs anonimizados o generados sint√©ticamente
- **Performance thresholds**: Configurar en k6 script:
  ```javascript
  export let options = {
    thresholds: {
      'http_req_duration': ['p(95)<30000'], // 95% under 30s
      'http_req_failed': ['rate<0.05'], // <5% failures
    }
  };
  ```
- **CI integration**: Usar GitHub Actions con service containers para MySQL, RabbitMQ, Redis
- **Flaky tests prevention**: Usar waiters apropiados (waitForSelector, waitForResponse), evitar sleeps arbitrarios
- **Parallelization**: Ejecutar E2E tests en paralelo con sharding (Playwright workers, Cypress parallel)

#### Artefactos
- [ ] `tests/e2e/cv-upload.spec.ts`
- [ ] `tests/e2e/cv-duplicate-detection.spec.ts`
- [ ] `tests/performance/cv-parsing-load.js`
- [ ] `tests/fixtures/sample_cvs/` (10+ CVs de diferentes formatos)
- [ ] `.github/workflows/e2e-tests.yml` (CI config)
- [ ] `tests/README.md` (gu√≠a de testing)
- [ ] `playwright.config.ts` o `cypress.config.ts`

---

### TICKET US-001-10: DevOps - Docker Compose & Local Development Setup

**Tipo**: Technical
**Componente**: DevOps
**Asignado a**: DevOps / FullStack Dev
**Estimaci√≥n**: 2 d√≠as

#### Descripci√≥n
Crear configuraci√≥n completa de Docker Compose para desarrollo local que incluya todos los servicios: frontend, backend, parser service, MySQL, RabbitMQ, Redis, S3 (MinIO). Incluye scripts de inicializaci√≥n y seeding.

#### Tareas T√©cnicas
- [ ] Crear `docker-compose.yml` con todos los servicios
- [ ] Service `mysql`: imagen oficial, volume para persistencia, init script para schema
- [ ] Service `rabbitmq`: imagen con management plugin, puertos 5672 (AMQP) y 15672 (UI)
- [ ] Service `redis`: imagen oficial, volume para persistencia (opcional)
- [ ] Service `minio`: S3-compatible storage, crear bucket `lti-cvs-dev` en startup
- [ ] Service `backend`: Node.js app, hot-reload con volumes, depends_on mysql+rabbitmq+redis
- [ ] Service `parser-service`: Python FastAPI, hot-reload con volumes, depends_on mysql+s3
- [ ] Service `frontend`: React app en dev mode, proxy a backend, hot-reload
- [ ] Health checks para todos los servicios (verificar ready antes de depends_on)
- [ ] Networking: todos los servicios en misma red `lti-network`
- [ ] Environment variables en `.env.example` con valores por defecto
- [ ] Script `scripts/dev-setup.sh` que ejecuta migrations, seeds, y crea bucket S3
- [ ] README con instrucciones paso a paso: `docker-compose up`, acceder a servicios

#### Criterios de Aceptaci√≥n T√©cnicos
- [ ] `docker-compose up` inicia todos los servicios sin errores
- [ ] Health checks pasan para todos los servicios en <2 minutos
- [ ] Frontend accesible en http://localhost:3000
- [ ] Backend API accesible en http://localhost:4000
- [ ] Parser Service accesible en http://localhost:8000
- [ ] RabbitMQ Management UI en http://localhost:15672 (user: guest, pass: guest)
- [ ] MinIO Console en http://localhost:9001 (user: minioadmin, pass: minioadmin)
- [ ] Hot-reload funciona: cambio en c√≥digo se refleja sin reiniciar contenedor
- [ ] Vol√∫menes persisten data: `docker-compose down` + `up` no pierde datos

#### Dependencias
- Depende de: US-001-01 (DB Schema), US-001-02 (Backend), US-001-05 (Parser), US-001-07 (Frontend)
- Bloquea a: Ninguno (habilita desarrollo local f√°cil)

#### Definici√≥n T√©cnica de Hecho
- [ ] `docker-compose.yml` configurado con todos los servicios
- [ ] `docker-compose.override.yml` para configuraciones de desarrollo
- [ ] `.env.example` con todas las variables necesarias
- [ ] `scripts/dev-setup.sh` ejecuta migrations y seeds
- [ ] `scripts/dev-teardown.sh` limpia volumes y data
- [ ] README actualizado con secci√≥n "Local Development Setup"
- [ ] Troubleshooting guide para problemas comunes
- [ ] Code review aprobado

#### Notas de Implementaci√≥n
- **Docker Compose version**: Usar v3.8+ (soporta health checks mejorados)
- **Volumes**: Nombrar volumes expl√≠citamente (no anonymous): `mysql_data`, `rabbitmq_data`, `minio_data`
- **Hot-reload**:
  - Backend: Volume mount `./backend:/app`, usar `nodemon` o `ts-node-dev`
  - Frontend: Volume mount `./frontend:/app`, usar `vite` o `webpack-dev-server`
  - Parser: Volume mount `./parser-service:/app`, usar `uvicorn --reload`
- **Health checks**:
  ```yaml
  healthcheck:
    test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
    interval: 10s
    timeout: 5s
    retries: 5
  ```
- **MinIO bucket creation**: Usar `mc` (MinIO Client) en init script o usar entrypoint script
- **Depends_on with condition**: Usar `condition: service_healthy` para ordenar startup
- **Network**: Crear custom network con `driver: bridge` para DNS interno

#### Artefactos
- [ ] `docker-compose.yml`
- [ ] `docker-compose.override.yml`
- [ ] `.env.example`
- [ ] `scripts/dev-setup.sh`
- [ ] `scripts/dev-teardown.sh`
- [ ] `docker/mysql/init.sql` (schema + seeds)
- [ ] `docker/minio/create-buckets.sh`
- [ ] `README.md` (updated with dev setup)
- [ ] `docs/TROUBLESHOOTING.md`

---

## üìä Diagrama de Dependencias entre Tickets

```mermaid
graph TD
    T01[US-001-01<br/>DB Schema]
    T02[US-001-02<br/>Upload API]
    T03[US-001-03<br/>Status API]
    T04[US-001-04<br/>RabbitMQ]
    T05[US-001-05<br/>Parser Service]
    T06[US-001-06<br/>NLP Training]
    T07[US-001-07<br/>Frontend UI]
    T08[US-001-08<br/>LinkedIn Scraper]
    T09[US-001-09<br/>E2E Tests]
    T10[US-001-10<br/>Docker Compose]

    T01 -->|Habilita| T02
    T01 -->|Habilita| T03
    T01 -->|Habilita| T05
    T02 -->|Habilita| T03
    T02 -->|Habilita| T04
    T02 -->|Habilita| T05
    T03 -->|Habilita| T07
    T04 -->|Habilita| T05
    T05 -->|Habilita| T06
    T05 -->|Habilita| T08
    T02 -->|Habilita| T07
    T07 -->|Habilita| T09
    T05 -->|Habilita| T09
    T01 -->|Habilita| T10
    T02 -->|Habilita| T10
    T05 -->|Habilita| T10
    T07 -->|Habilita| T10

    style T01 fill:#e8f5e9
    style T02 fill:#fff3e0
    style T03 fill:#fff3e0
    style T04 fill:#f3e5f5
    style T05 fill:#f3e5f5
    style T06 fill:#e1f5fe
    style T07 fill:#e1f5ff
    style T08 fill:#e1f5fe
    style T09 fill:#fce4ec
    style T10 fill:#e0f2f1
```

**Leyenda de colores:**
- üü¢ Verde: Database/Infraestructura base
- üü† Naranja: Backend APIs
- üü£ Morado: Servicios de procesamiento
- üîµ Azul: AI/ML y Features avanzadas
- üî¥ Rosa: Testing
- ‚ö™ Cian: DevOps

---

## üóìÔ∏è Orden de Implementaci√≥n Sugerido

### Semana 1 (D√≠as 1-5)

**Track 1: Backend + Database**
1. **D√≠a 1-2**: US-001-01 (Database Schema)
2. **D√≠a 3-5**: US-001-02 (Upload API)

**Track 2: Parser Service**
1. **D√≠a 1-5**: US-001-05 (Parser Service Core) - Iniciar en paralelo

### Semana 2 (D√≠as 6-10)

**Track 1: Backend APIs**
1. **D√≠a 6-7**: US-001-03 (Status & Results API)
2. **D√≠a 8-9**: US-001-04 (RabbitMQ Setup)

**Track 2: Frontend**
1. **D√≠a 6-9**: US-001-07 (Frontend UI)

**Track 3: DevOps**
1. **D√≠a 8-9**: US-001-10 (Docker Compose)

### Semana 3 (D√≠as 11-15)

**Polish & Enhancement**
1. **D√≠a 11-13**: US-001-06 (NLP Training) - Opcional, si hay tiempo
2. **D√≠a 11-13**: US-001-08 (LinkedIn Scraper) - Opcional, si hay tiempo
3. **D√≠a 13-15**: US-001-09 (E2E & Performance Tests)

**Milestone**: US-001 completada y lista para integraci√≥n con US-005

---

## ‚ö†Ô∏è Riesgos T√©cnicos Identificados

| # | Riesgo | Probabilidad | Impacto | Mitigaci√≥n |
|---|--------|--------------|---------|------------|
| 1 | **Precisi√≥n de NLP <85%** en campos cr√≠ticos | Media | Alto | - Entrenar modelo custom (US-001-06)<br/>- Fallback a revisi√≥n manual<br/>- Validaciones adicionales client-side |
| 2 | **Parsing de PDFs complejos** (multi-columna, escaneados) falla | Alta | Medio | - Implementar OCR con AWS Textract<br/>- Soportar edici√≥n manual de todos los campos<br/>- Iteraci√≥n incremental de parsers |
| 3 | **Rate limits de LinkedIn** o cambios en estructura HTML | Alta | Medio | - Priorizar API oficial si disponible<br/>- Implementar rate limiting agresivo<br/>- Cache de profiles<br/>- Fallback a upload manual de CV |
| 4 | **Performance del Parser Service** <30s no se cumple para CVs largos | Media | Medio | - Optimizar parsers (lazy loading, streaming)<br/>- Timeouts configurables<br/>- Process en background con notifications<br/>- Sharding de jobs pesados |
| 5 | **Costo de AWS Textract** se dispara con muchos PDFs escaneados | Media | Bajo | - Detectar PDFs escaneados antes de enviar a OCR<br/>- Limitar OCR a X requests/d√≠a<br/>- Usar alternativa open-source (Tesseract) primero |
| 6 | **Normalizaci√≥n de skills** inconsistente (sin√≥nimos no capturados) | Alta | Bajo | - Diccionario extenso con crowdsourcing<br/>- Algoritmo de fuzzy matching<br/>- Permitir edici√≥n manual siempre<br/>- Iteraci√≥n continua del diccionario |
| 7 | **Dependencia de servicios externos** (S3, Textract, RabbitMQ) causa downtime | Media | Alto | - Implementar circuit breakers<br/>- Health checks robustos<br/>- Fallbacks y retries con exponential backoff<br/>- Monitoring y alerting |
| 8 | **Duplicados no detectados** correctamente (falsos negativos) | Media | Medio | - Algoritmo de detecci√≥n multi-factor (email + fuzzy name)<br/>- UI clara de duplicados para revisi√≥n manual<br/>- Merge de duplicados post-facto |

---

## üìà M√©tricas de √âxito

### M√©tricas T√©cnicas
- **Parsing Accuracy**: >85% en campos cr√≠ticos (nombre, email, skills)
- **Processing Time**: <30 segundos para 95% de CVs (<5 p√°ginas)
- **Uptime**: >99% de disponibilidad del servicio de upload
- **Error Rate**: <5% de parsings fallidos (excluyendo PDFs inv√°lidos)
- **Throughput**: Capacidad de procesar >100 CVs/hora concurrentemente

### M√©tricas de Calidad de C√≥digo
- **Test Coverage**: >80% en backend, >75% en frontend, >80% en parser
- **Code Review**: 100% de PRs revisados antes de merge
- **Security Scans**: 0 vulnerabilidades cr√≠ticas (Snyk, npm audit)
- **Performance Budgets**: Time to Interactive <3s en frontend

### M√©tricas de UX
- **User Satisfaction**: NPS >7 en flujo de upload (medido con beta users)
- **Error Recovery**: Tasa de √©xito >90% despu√©s de primera edici√≥n manual
- **Time to Completion**: Usuario completa upload + confirmaci√≥n en <2 minutos

---

## üîÑ Siguientes Pasos

1. **Refinamiento con Equipo**: Revisar tickets en planning session, ajustar estimaciones
2. **Asignaci√≥n**: Asignar tickets a desarrolladores seg√∫n expertise
3. **Sprint Planning**: Distribuir tickets en Sprints 1-2 seg√∫n roadmap
4. **Kickoff T√©cnico**: Architecture review session antes de comenzar
5. **Daily Standups**: Trackear progreso y bloqueadores diariamente
6. **Mid-Sprint Review**: D√≠a 5 - revisar progreso y ajustar si es necesario
7. **Demo & Retrospective**: Final de Sprint 1 - demo de US-001 completa

---

## üìä Estimaciones de Esfuerzo

### Metodolog√≠a y Referencias de Calibraci√≥n

Este an√°lisis aplica **tres metodolog√≠as complementarias** de estimaci√≥n √°gil para garantizar precisi√≥n y consenso:

#### 1. Story Points (Fibonacci)
**Escala**: 1, 2, 3, 5, 8, 13, 21

**Referencias de Calibraci√≥n para este Team:**
- **1 punto** (~2-4 horas): Cambio trivial en c√≥digo existente, configuraci√≥n simple, sin incertidumbre
  - Ejemplo: Actualizar una variable de entorno, agregar un campo simple a una tabla
- **2 puntos** (~4-8 horas): Tarea peque√±a con l√≥gica simple, requiere testing b√°sico
  - Ejemplo: Crear un endpoint GET b√°sico con query simple, componente UI de solo lectura
- **3 puntos** (~1 d√≠a): Implementaci√≥n est√°ndar con l√≥gica moderada, testing unitario y de integraci√≥n
  - Ejemplo: CRUD completo de una entidad, formulario con validaciones
- **5 puntos** (~2-3 d√≠as): Complejidad media, algunos unknowns, m√∫ltiples archivos afectados, integraci√≥n con servicios externos
  - Ejemplo: Implementar autenticaci√≥n JWT, integraci√≥n con API externa con manejo de errores
- **8 puntos** (~1 semana): Alta complejidad, muchas dependencias, incertidumbre t√©cnica, requiere investigaci√≥n
  - Ejemplo: Implementar feature completo con backend + frontend + tests, l√≥gica de negocio compleja
- **13 puntos** (~2 semanas): Muy complejo, m√∫ltiples sistemas, alto riesgo, considerar subdividir
  - Ejemplo: Sistema de procesamiento as√≠ncrono completo con queue, workers, monitoring

**Factores considerados:**
- Complejidad t√©cnica (algoritmos, arquitectura)
- Incertidumbre/riesgos (tecnolog√≠as nuevas, dependencias externas)
- Esfuerzo de testing (unitario, integraci√≥n, E2E)
- Deuda t√©cnica generada (refactoring futuro necesario)

#### 2. T-Shirt Sizing
**Escala**: XS, S, M, L, XL

**Mapeo aproximado**:
- **XS** (< 4 horas): Trivial, sin riesgo
- **S** (4-8 horas): Simple, riesgo m√≠nimo
- **M** (1-2 d√≠as): Est√°ndar, riesgo controlado
- **L** (3-5 d√≠as): Complejo, riesgo medio
- **XL** (1-2 semanas): Muy complejo, alto riesgo, considerar subdividir

#### 3. Horas Ideales ‚Üí Horas Reales
**Horas Ideales**: Tiempo de trabajo sin interrupciones, en flujo perfecto

**Factor de Realidad**: **1.7x** (basado en experiencia del team)
- Meetings diarios (~1h/d√≠a)
- Code reviews y PR feedback
- Context switching
- Debugging y troubleshooting inesperado
- Documentation
- Interrupciones y comunicaci√≥n

**F√≥rmula**: Horas Reales = Horas Ideales √ó 1.7

**Asunciones del Team:**
- D√≠a laboral: 8 horas
- Horas productivas efectivas: ~5-6 horas/d√≠a
- 1 dev-d√≠a = ~6 horas efectivas
- Team: 3 full-stack devs (1 senior, 2 mid-level)

---

### Tabla de Estimaciones Consolidada

| Ticket ID | T√≠tulo | Story Points | T-Shirt | Horas Ideales | Horas Reales | Dev-D√≠as | Confianza | Riesgos Principales |
|-----------|--------|--------------|---------|---------------|--------------|----------|-----------|---------------------|
| **US-001-01** | Database Schema | **5** | **M** | 12h | 20h | 2.5 d√≠as | **Alto** | Schema changes post-deployment, √≠ndices no optimizados |
| **US-001-02** | File Upload API | **8** | **L** | 18h | 31h | 3.9 d√≠as | **Medio** | S3 upload failures, rate limiting edge cases |
| **US-001-03** | Job Status API | **5** | **M** | 10h | 17h | 2.1 d√≠as | **Alto** | Fuzzy matching performance, WebSocket complexity |
| **US-001-04** | RabbitMQ Setup | **5** | **M** | 12h | 20h | 2.5 d√≠as | **Medio** | Message loss en crash, retry logic bugs |
| **US-001-05** | Parser Service Core | **13** | **XL** | 32h | 54h | 6.8 d√≠as | **Bajo** | NLP accuracy, PDF parsing, performance <30s |
| **US-001-06** | NLP Training | **8** | **L** | 20h | 34h | 4.3 d√≠as | **Bajo** | Dataset quality, model accuracy, training time |
| **US-001-07** | Frontend UI | **8** | **L** | 22h | 37h | 4.6 d√≠as | **Medio** | Polling logic, responsive design, accessibility |
| **US-001-08** | LinkedIn Scraper | **8** | **L** | 18h | 31h | 3.9 d√≠as | **Bajo** | API availability, rate limits, CAPTCHA, legal |
| **US-001-09** | E2E & Perf Tests | **5** | **M** | 16h | 27h | 3.4 d√≠as | **Medio** | Flaky tests, performance thresholds, CI setup |
| **US-001-10** | Docker Compose | **3** | **M** | 10h | 17h | 2.1 d√≠as | **Alto** | Service dependencies, hot-reload issues |
| **TOTAL** | **US-001 Completo** | **68** | - | **170h** | **289h** | **36 d√≠as** | - | - |

**Resumen de Capacidad:**
- **Total Story Points**: 68 pts
- **Total Horas Reales**: 289 horas (~36 dev-d√≠as)
- **Duraci√≥n con 3 devs en paralelo**: ~3.2 semanas (asumiendo paralelizaci√≥n √≥ptima)
- **Duraci√≥n secuencial (cr√≠tica path)**: ~5 semanas
- **Velocidad necesaria**: 68 pts / 3.2 semanas = ~21 pts/semana (team de 3 devs)

---

### An√°lisis Detallado por Ticket

#### US-001-01: Database Schema - Candidate & Skills Tables

**üìä Estimaciones:**
- **Story Points**: **5** (Complejidad media)
- **T-Shirt Size**: **M** (1-2 d√≠as)
- **Horas Ideales**: 12h ‚Üí **Horas Reales**: 20h (~2.5 dev-d√≠as)

**Justificaci√≥n de Story Points (5):**
- **Complejidad T√©cnica** (2/5): Schema design es est√°ndar, pero requiere decisiones de normalizaci√≥n y tipos de datos JSON
- **Incertidumbre** (1/5): Baja incertidumbre, patr√≥n bien conocido
- **Testing** (1/5): Tests de constraints, migrations, √≠ndices
- **Deuda T√©cnica** (1/5): Schema changes futuras son costosas, debe estar bien dise√±ado

**Factores de Complejidad:**
- Dise√±o de relaciones many-to-many (candidate_skills)
- Decisi√≥n de tipos JSON vs columnas estructuradas
- Dise√±o de √≠ndices compuestos para queries eficientes
- Normalizaci√≥n de skills con synonyms
- Soft deletes y auditor√≠a
- Seed data de 200+ skills

**Riesgos de Estimaci√≥n:**
- ‚ö†Ô∏è **Cambios en schema post-deployment**: Dif√≠ciles de migrar, requiere planificaci√≥n cuidadosa (mitigaci√≥n: review exhaustivo antes de merge)
- ‚ö†Ô∏è **Performance de √≠ndices**: √çndices mal dise√±ados pueden no detectarse hasta carga real (mitigaci√≥n: EXPLAIN queries en tests)
- ‚ö†Ô∏è **JSON schema validation**: MySQL 8.0 tiene soporte limitado vs PostgreSQL (mitigaci√≥n: validar en application layer tambi√©n)

**Supuestos:**
- Equipo tiene experiencia con MySQL y ORMs (TypeORM/Knex)
- No hay requirements de particionamiento en MVP
- Seed data de skills puede ser semi-automatizada (scraping de job boards)

**Nivel de Confianza**: **Alto (90%)** - Task bien definida, stack conocido, pocas inc√≥gnitas

---

#### US-001-02: Backend API - File Upload Endpoints

**üìä Estimaciones:**
- **Story Points**: **8** (Alta complejidad)
- **T-Shirt Size**: **L** (3-5 d√≠as)
- **Horas Ideales**: 18h ‚Üí **Horas Reales**: 31h (~3.9 dev-d√≠as)

**Justificaci√≥n de Story Points (8):**
- **Complejidad T√©cnica** (3/5): Upload multipart, streaming a S3, hash calculation, deduplicaci√≥n
- **Incertidumbre** (2/5): Integraci√≥n con S3, manejo de errores edge cases, rate limiting
- **Testing** (2/5): Tests unitarios, integraci√≥n con S3 mock, tests de security
- **Deuda T√©cnica** (1/5): C√≥digo de infra que debe ser robusto desde el inicio

**Factores de Complejidad:**
- Configuraci√≥n de multer con validaciones de seguridad
- Streaming de archivos grandes a S3 sin cargar en memoria
- C√°lculo de hash SHA-256 eficiente
- Deduplicaci√≥n por hash con query a cache
- Generaci√≥n de presigned URLs con expiraci√≥n
- Integraci√≥n con RabbitMQ producer
- Validaci√≥n de MIME types reales (no solo extensi√≥n)
- Rate limiting por usuario
- Manejo de errores y retries con S3

**Riesgos de Estimaci√≥n:**
- ‚ö†Ô∏è **S3 upload failures**: Network issues, timeouts, permissions (mitigaci√≥n: retry con exponential backoff)
- ‚ö†Ô∏è **Security vulnerabilities**: MIME type spoofing, path traversal (mitigaci√≥n: security audit, penetration testing)
- ‚ö†Ô∏è **Rate limiting edge cases**: Distributed systems, Redis para contadores (mitigaci√≥n: usar biblioteca probada como `express-rate-limit`)
- ‚ö†Ô∏è **File size validation**: Evasi√≥n de l√≠mites con chunked uploads (mitigaci√≥n: validar tanto client-side como server-side)

**Supuestos:**
- AWS SDK v3 setup ya configurado (credentials, buckets creados)
- RabbitMQ producer service ya existe o se desarrolla en paralelo (US-001-04)
- MinIO funciona como drop-in replacement de S3 para local dev
- LinkedIn URL validation es simple regex (no scraping a√∫n)

**Nivel de Confianza**: **Medio (70%)** - M√∫ltiples integraciones externas, edge cases de seguridad

---

#### US-001-03: Backend API - Job Status & Results Endpoints

**üìä Estimaciones:**
- **Story Points**: **5** (Complejidad media)
- **T-Shirt Size**: **M** (1-2 d√≠as)
- **Horas Ideales**: 10h ‚Üí **Horas Reales**: 17h (~2.1 dev-d√≠as)

**Justificaci√≥n de Story Points (5):**
- **Complejidad T√©cnica** (2/5): CRUD est√°ndar con l√≥gica de fuzzy matching
- **Incertidumbre** (1/5): Algoritmo de duplicados puede requerir tuning
- **Testing** (1/5): Tests unitarios para fuzzy logic, integraci√≥n para endpoints
- **Deuda T√©cnica** (1/5): C√≥digo relativamente simple, poco impacto futuro

**Factores de Complejidad:**
- Endpoints CRUD est√°ndar para jobs y parsed data
- Implementaci√≥n de fuzzy matching (Levenshtein distance)
- Detecci√≥n de duplicados con m√∫ltiples criterios
- Ediciones manuales con tracking de cambios
- Confirmaci√≥n con evento para downstream services
- Caching con Redis
- WebSocket opcional para real-time updates

**Riesgos de Estimaci√≥n:**
- ‚ö†Ô∏è **Fuzzy matching performance**: Algoritmo Levenshtein O(n¬≤) puede ser lento con 10k+ candidatos (mitigaci√≥n: pre-filtrar por primera letra, usar √≠ndices)
- ‚ö†Ô∏è **WebSocket complexity**: Si se implementa, agrega complejidad significativa (mitigaci√≥n: marcar como opcional para MVP)
- ‚ö†Ô∏è **Race conditions**: Ediciones concurrentes pueden causar data loss (mitigaci√≥n: optimistic locking con `updated_at`)

**Supuestos:**
- WebSocket es opcional y puede omitirse en MVP (polling es suficiente)
- Algoritmo de fuzzy matching con threshold fijo (no ML)
- Duplicate detection es best-effort (puede tener falsos negativos/positivos)

**Nivel de Confianza**: **Alto (85%)** - Funcionalidad est√°ndar, complejidad controlada

---

#### US-001-04: Message Queue - RabbitMQ Setup & Consumers

**üìä Estimaciones:**
- **Story Points**: **5** (Complejidad media)
- **T-Shirt Size**: **M** (1-2 d√≠as)
- **Horas Ideales**: 12h ‚Üí **Horas Reales**: 20h (~2.5 dev-d√≠as)

**Justificaci√≥n de Story Points (5):**
- **Complejidad T√©cnica** (2/5): Setup de RabbitMQ, exchanges, queues, DLQ
- **Incertidumbre** (2/5): Retry logic con exponential backoff puede tener edge cases
- **Testing** (1/5): Tests de integraci√≥n con RabbitMQ, simulaci√≥n de failures
- **Deuda T√©cnica** (0/5): Infraestructura cr√≠tica, debe ser robusta

**Factores de Complejidad:**
- Configuraci√≥n de RabbitMQ en Docker Compose
- Dise√±o de exchanges y bindings
- Dead letter queues para mensajes fallidos
- Producer con confirmations
- Consumer con manual ACK
- Retry logic con headers y TTL
- Exponential backoff (1min, 5min, 15min)
- Graceful shutdown con mensajes en flight
- Monitoring de queue depth

**Riesgos de Estimaci√≥n:**
- ‚ö†Ô∏è **Message loss en crash**: Si consumer crashea antes de ACK, mensaje puede perderse (mitigaci√≥n: durable queues, persistent messages, manual ACK)
- ‚ö†Ô∏è **Retry logic bugs**: Mensajes pueden quedar en loop infinito o ir a DLQ prematuramente (mitigaci√≥n: tests exhaustivos con diferentes escenarios de fallo)
- ‚ö†Ô∏è **Connection recovery**: Reconexi√≥n autom√°tica puede fallar en escenarios complejos (mitigaci√≥n: usar biblioteca con auto-recovery probada)

**Supuestos:**
- RabbitMQ es el message broker elegido (no Kafka, SQS, etc.)
- Consumer simple (solo delega a Parser Service HTTP call)
- No se requiere partitioning o sharding en MVP
- Monitoring b√°sico es suficiente (no Prometheus/Grafana a√∫n)

**Nivel de Confianza**: **Medio (75%)** - Tecnolog√≠a madura pero retry logic puede ser tricky

---

#### US-001-05: Parser Service - Python FastAPI Core

**üìä Estimaciones:**
- **Story Points**: **13** (Muy alta complejidad)
- **T-Shirt Size**: **XL** (1-2 semanas)
- **Horas Ideales**: 32h ‚Üí **Horas Reales**: 54h (~6.8 dev-d√≠as)

**Justificaci√≥n de Story Points (13):**
- **Complejidad T√©cnica** (5/5): M√∫ltiples parsers (PDF, DOCX, TXT), NLP con spaCy, normalizaci√≥n, caching
- **Incertidumbre** (4/5): Precisi√≥n de NLP, performance <30s no garantizada, PDFs complejos
- **Testing** (2/5): Tests unitarios por parser, integraci√≥n, performance testing
- **Deuda T√©cnica** (2/5): C√≥digo complejo que requerir√° refactoring iterativo

**Factores de Complejidad:**
- Desarrollo de servicio FastAPI completo desde cero
- Implementaci√≥n de 3 parsers diferentes (PDF, DOCX, TXT)
- Extracci√≥n de texto con PyPDF2, pdfplumber, python-docx
- Encoding detection con chardet
- NLP extraction con spaCy NER (nombre, email, tel√©fono, ubicaci√≥n, skills, educaci√≥n, experiencia)
- Custom entity recognizer para skills
- Normalizaci√≥n de skills con diccionario externo (HTTP call a backend)
- C√°lculo de confidence scores por campo
- OCR fallback con AWS Textract para PDFs escaneados
- Caching con Redis por file_hash
- Manejo de errores y logging
- Performance optimization para <10s en CV simple

**Riesgos de Estimaci√≥n:**
- üî¥ **NLP accuracy <85%**: spaCy base models pueden no ser precisos para CVs t√©cnicos (mitigaci√≥n: US-001-06 custom training, fallback a keyword matching)
- üî¥ **PDF parsing failures**: Multi-columna, tablas, formatos complejos (mitigaci√≥n: pdfplumber para tablas, OCR fallback, permitir edici√≥n manual)
- üî¥ **Performance <30s no cumplido**: CVs largos (>10 p√°ginas), spaCy lento (mitigaci√≥n: lazy loading de modelos, streaming, timeouts configurables, procesamiento incremental)
- ‚ö†Ô∏è **OCR cost**: AWS Textract $1.50/1000 pages puede ser caro (mitigaci√≥n: detectar PDFs escaneados antes de OCR, usar Tesseract primero)
- ‚ö†Ô∏è **Skills extraction inconsistent**: Skills con m√∫ltiples nombres (React, ReactJS, React.js) (mitigaci√≥n: diccionario extenso, fuzzy matching)

**Supuestos:**
- Developer tiene experiencia con Python y NLP (si no, +30% tiempo)
- spaCy models (`es_core_news_lg`, `en_core_web_lg`) son suficientemente buenos para MVP
- Custom NER training (US-001-06) es opcional para MVP
- OCR fallback es edge case (<10% de CVs)
- Normalizaci√≥n de skills puede tener errores (best-effort en MVP)

**Nivel de Confianza**: **Bajo (60%)** - Ticket m√°s complejo, m√∫ltiples unknowns, dependencia de calidad de NLP

**Recomendaci√≥n**: Considerar **spike de 1 d√≠a** antes de comenzar implementaci√≥n para:
- Validar precisi√≥n de spaCy con 10 CVs reales
- Probar parsing de PDFs complejos
- Medir performance baseline

---

#### US-001-06: NLP Training - Custom spaCy Entity Recognizer

**üìä Estimaciones:**
- **Story Points**: **8** (Alta complejidad)
- **T-Shirt Size**: **L** (3-5 d√≠as)
- **Horas Ideales**: 20h ‚Üí **Horas Reales**: 34h (~4.3 dev-d√≠as)

**Justificaci√≥n de Story Points (8):**
- **Complejidad T√©cnica** (3/5): Training de modelo NER, evaluaci√≥n, versionado
- **Incertidumbre** (3/5): Calidad de dataset, accuracy del modelo, iteraciones necesarias
- **Testing** (1/5): Evaluation en test set, comparaci√≥n con baseline
- **Deuda T√©cnica** (1/5): Training pipeline debe ser reproducible y versionado

**Factores de Complejidad:**
- Recolecci√≥n de dataset: 100+ CVs anonimizados
- Anotaci√≥n manual con herramienta (Label Studio, Prodigy, Doccano)
- Etiquetado de entidades: SKILL, JOB_TITLE, COMPANY, DEGREE, CERTIFICATION
- Exportaci√≥n en formato spaCy training (JSON)
- Implementaci√≥n de training script con spaCy API
- Fine-tuning de modelos pre-entrenados
- Evaluaci√≥n con precision, recall, F1-score
- Iteraci√≥n hasta alcanzar F1 >0.80 para SKILL
- Versionado con MLflow o DVC
- Integraci√≥n en Parser Service

**Riesgos de Estimaci√≥n:**
- üî¥ **Dataset quality**: 100 CVs pueden no ser suficientes, anotaciones inconsistentes (mitigaci√≥n: inter-annotator agreement, m√°s datos si es necesario)
- üî¥ **Model accuracy**: F1 >0.80 puede no lograrse con 100 CVs (mitigaci√≥n: data augmentation, m√°s iteraciones, considerar BERT)
- ‚ö†Ô∏è **Training time**: M√∫ltiples iteraciones pueden consumir m√°s tiempo (mitigaci√≥n: usar GPU si disponible, empezar con subset peque√±o)
- ‚ö†Ô∏è **Annotation effort**: Etiquetar 100 CVs puede tomar 10+ horas (mitigaci√≥n: dividir entre equipo, usar pre-annotations)

**Supuestos:**
- Dataset de 100 CVs es suficiente para MVP (puede necesitar m√°s iteraciones)
- Herramientas de anotaci√≥n son open-source (Label Studio) o ya hay licencia (Prodigy)
- Fine-tuning es suficiente (no se requiere training from scratch)
- Target F1 >0.80 es realista con dataset small
- Developer tiene experiencia con ML training (si no, +50% tiempo)

**Nivel de Confianza**: **Bajo (65%)** - Alta dependencia de calidad de datos, iteraciones impredecibles

**Recomendaci√≥n**: Marcar como **opcional para MVP**. Primero validar accuracy de modelo base (US-001-05), solo entrenar custom si accuracy <70%

---

#### US-001-07: Frontend UI - CV Upload Component

**üìä Estimaciones:**
- **Story Points**: **8** (Alta complejidad)
- **T-Shirt Size**: **L** (3-5 d√≠as)
- **Horas Ideales**: 22h ‚Üí **Horas Reales**: 37h (~4.6 dev-d√≠as)

**Justificaci√≥n de Story Points (8):**
- **Complejidad T√©cnica** (3/5): Drag-and-drop, polling, edici√≥n de campos, detecci√≥n de duplicados
- **Incertidumbre** (2/5): Polling strategy, responsive design, accesibilidad
- **Testing** (2/5): Tests unitarios, integraci√≥n con MSW, tests de accesibilidad
- **Deuda T√©cnica** (1/5): C√≥digo de UI que puede requerir refactoring para UX

**Factores de Complejidad:**
- Componente `CVUploadZone` con drag-and-drop (react-dropzone)
- Validaci√≥n client-side (tipo, tama√±o)
- Upload con progress bar (axios onUploadProgress)
- Polling de job status cada 2 segundos
- Componente `ParsedDataPreview` con m√∫ltiples sections
- Highlight de campos con baja confianza (<70%)
- Componente `EditableField` para edici√≥n manual
- Modal de duplicados con opciones
- Manejo de errores y loading states
- Responsive design (desktop + tablet)
- Accesibilidad (keyboard navigation, aria-labels)
- Storybook stories para cada componente

**Riesgos de Estimaci√≥n:**
- ‚ö†Ô∏è **Polling logic bugs**: Memory leaks, polling no se detiene, race conditions (mitigaci√≥n: usar react-query con refetchInterval, cleanup en useEffect)
- ‚ö†Ô∏è **Responsive design complexity**: Multi-device testing puede revelar issues (mitigaci√≥n: usar mobile-first approach, testing en BrowserStack)
- ‚ö†Ô∏è **Accessibility violations**: axe-core puede encontrar violations que requieren refactoring (mitigaci√≥n: tests de accesibilidad desde el inicio)
- ‚ö†Ô∏è **Progress bar UX**: Progress reporting inexacto puede confundir usuarios (mitigaci√≥n: usar estimaciones conservadoras, messaging claro)

**Supuestos:**
- React 19.2 no tiene breaking changes vs 18 (o equipo ya conoce diferencias)
- UI library (Material-UI o Chakra UI) ya est√° configurada en proyecto
- Backend APIs (US-001-02, US-001-03) ya est√°n funcionando o se mockean con MSW
- Storybook ya est√° configurado en proyecto

**Nivel de Confianza**: **Medio (75%)** - Funcionalidad est√°ndar pero m√∫ltiples sub-componentes

---

#### US-001-08: Integration - LinkedIn Profile Scraper

**üìä Estimaciones:**
- **Story Points**: **8** (Alta complejidad)
- **T-Shirt Size**: **L** (3-5 d√≠as)
- **Horas Ideales**: 18h ‚Üí **Horas Reales**: 31h (~3.9 dev-d√≠as)

**Justificaci√≥n de Story Points (8):**
- **Complejidad T√©cnica** (3/5): Scraping con playwright, parsing HTML, rate limiting
- **Incertidumbre** (4/5): API availability, anti-bot, estructura HTML cambia, legal compliance
- **Testing** (1/5): Tests con HTML fixtures, integration tests con profiles reales
- **Deuda T√©cnica** (0/5): C√≥digo fr√°gil que requerir√° mantenimiento continuo

**Factores de Complejidad:**
- Investigaci√≥n de LinkedIn API oficial (puede no estar disponible)
- Implementaci√≥n de scraper con playwright/selenium
- Extracci√≥n de datos: nombre, headline, ubicaci√≥n, empresa, educaci√≥n, skills, experiencia
- Parsing HTML con BeautifulSoup/lxml
- Rate limiting: max 10 profiles/minuto
- Manejo de CAPTCHA (detecci√≥n, fallback a manual review)
- Detecci√≥n de perfiles privados/no accesibles
- Normalizaci√≥n de datos al formato est√°ndar
- Caching de profiles por URL
- Error handling robusto

**Riesgos de Estimaci√≥n:**
- üî¥ **LinkedIn API no disponible**: Requiere partnership, puede no ser viable (mitigaci√≥n: usar scraping, pero tiene riesgos legales)
- üî¥ **Anti-bot detection**: LinkedIn puede bloquear scraping (mitigaci√≥n: rate limiting agresivo, User-Agent realista, considerar proxies)
- üî¥ **HTML structure changes**: LinkedIn cambia frecuentemente (mitigaci√≥n: usar selectors robustos, monitoring de cambios, fallback a manual)
- üî¥ **Legal compliance**: Scraping puede violar ToS de LinkedIn (mitigaci√≥n: legal review, solo datos p√∫blicos, rate limiting)
- ‚ö†Ô∏è **CAPTCHA blocking**: Puede requerir intervenci√≥n manual frecuente (mitigaci√≥n: marcar como "manual_review_required", considerar servicio de CAPTCHA solving)

**Supuestos:**
- LinkedIn API oficial NO est√° disponible (scraping es necesario)
- Scraping de datos p√∫blicos es legal en jurisdicci√≥n del proyecto (requiere legal review)
- Rate limiting de 10 profiles/minuto es suficientemente conservador
- CAPTCHA es edge case (<10% de requests)
- HTML structure permanece estable por al menos 3-6 meses

**Nivel de Confianza**: **Bajo (55%)** - Alto riesgo t√©cnico y legal, dependencia de servicio externo no controlado

**Recomendaci√≥n**: Marcar como **OPCIONAL para MVP**. Considerar primero solo upload de PDF/DOCX. Si es cr√≠tico, hacer **spike de 1 d√≠a** para validar viabilidad t√©cnica y legal.

---

#### US-001-09: Testing - E2E & Performance Tests

**üìä Estimaciones:**
- **Story Points**: **5** (Complejidad media)
- **T-Shirt Size**: **M** (1-2 d√≠as)
- **Horas Ideales**: 16h ‚Üí **Horas Reales**: 27h (~3.4 dev-d√≠as)

**Justificaci√≥n de Story Points (5):**
- **Complejidad T√©cnica** (2/5): Setup de Playwright/Cypress, implementaci√≥n de tests E2E y performance
- **Incertidumbre** (2/5): Flaky tests, configuraci√≥n de CI con service containers
- **Testing** (0/5): Este ticket ES testing
- **Deuda T√©cnica** (1/5): Tests requieren mantenimiento continuo

**Factores de Complejidad:**
- Setup de Playwright o Cypress para E2E
- Implementaci√≥n de 4+ test scenarios (happy path + errors)
- Performance tests con k6 o Artillery
- Load test: 50 usuarios concurrentes
- Throughput test: 100 CVs
- Smoke tests para deployment
- Configuraci√≥n de CI/CD (GitHub Actions)
- Service containers para MySQL, RabbitMQ, Redis en CI
- Test data fixtures (10+ CVs)
- Screenshots/videos en fallos
- Reportes HTML

**Riesgos de Estimaci√≥n:**
- ‚ö†Ô∏è **Flaky tests**: Tests E2E pueden fallar intermitentemente (mitigaci√≥n: waiters apropiados, retries, isolation de tests)
- ‚ö†Ô∏è **CI setup complexity**: Service containers en GitHub Actions pueden tener issues (mitigaci√≥n: usar Docker Compose en CI, debugging incremental)
- ‚ö†Ô∏è **Performance thresholds**: SLA de <30s puede no cumplirse inicialmente (mitigaci√≥n: marcar como warning, no blocker, iterar)
- ‚ö†Ô∏è **Test data management**: CVs reales pueden contener PII (mitigaci√≥n: usar CVs sint√©ticos o anonimizados)

**Supuestos:**
- Frontend y backend ya est√°n completamente implementados
- Tests E2E se ejecutan en ambiente integrado (no producci√≥n)
- Performance tests usan datos sint√©ticos (no producci√≥n)
- CI/CD pipeline (GitHub Actions) es la plataforma elegida

**Nivel de Confianza**: **Medio (75%)** - Complejidad est√°ndar, pero flaky tests y CI setup pueden consumir tiempo extra

---

#### US-001-10: DevOps - Docker Compose & Local Development Setup

**üìä Estimaciones:**
- **Story Points**: **3** (Complejidad baja-media)
- **T-Shirt Size**: **M** (1-2 d√≠as)
- **Horas Ideales**: 10h ‚Üí **Horas Reales**: 17h (~2.1 dev-d√≠as)

**Justificaci√≥n de Story Points (3):**
- **Complejidad T√©cnica** (1/5): Configuraci√≥n de Docker Compose con servicios existentes
- **Incertidumbre** (1/5): Hot-reload puede tener issues, depends_on timing
- **Testing** (0/5): Testing es manual (verificar que servicios levantan)
- **Deuda T√©cnica** (1/5): Docker Compose requiere mantenimiento cuando se agregan servicios

**Factores de Complejidad:**
- Creaci√≥n de `docker-compose.yml` con 8 servicios
- Configuraci√≥n de volumes para persistencia
- Health checks para cada servicio
- Depends_on con condition: service_healthy
- Hot-reload para backend, frontend, parser service
- Network configuration
- `.env.example` con variables
- Scripts de inicializaci√≥n (migrations, seeds, buckets)
- Script de teardown
- README con instrucciones

**Riesgos de Estimaci√≥n:**
- ‚ö†Ô∏è **Service dependencies timing**: depends_on puede no garantizar que servicio est√© ready (mitigaci√≥n: usar health checks, wait-for scripts)
- ‚ö†Ô∏è **Hot-reload issues**: Volumes en Windows pueden tener performance issues (mitigaci√≥n: documentar workarounds, considerar WSL2)
- ‚ö†Ô∏è **Port conflicts**: Puertos pueden estar ocupados en m√°quinas de developers (mitigaci√≥n: usar puertos no est√°ndar, documentar)

**Supuestos:**
- Todos los servicios ya tienen Dockerfiles (frontend, backend, parser)
- Equipo tiene experiencia con Docker y Docker Compose
- Desarrollo en Linux/Mac (Windows puede tener issues adicionales)
- No se requiere orchestration complejo (no Kubernetes en local)

**Nivel de Confianza**: **Alto (90%)** - Task bien definida, riesgos controlados

---

### Resumen de Capacidad y Planificaci√≥n

#### Distribuci√≥n de Story Points por Tipo

| Tipo | Tickets | Story Points | % Total |
|------|---------|--------------|---------|
| **Backend** | US-001-02, US-001-03, US-001-04 | 18 pts | 26% |
| **AI/ML** | US-001-05, US-001-06 | 21 pts | 31% |
| **Frontend** | US-001-07 | 8 pts | 12% |
| **Integration** | US-001-08 | 8 pts | 12% |
| **Testing** | US-001-09 | 5 pts | 7% |
| **DevOps** | US-001-10 | 3 pts | 4% |
| **Database** | US-001-01 | 5 pts | 7% |
| **TOTAL** | 10 tickets | **68 pts** | 100% |

#### Distribuci√≥n de Confianza

| Nivel de Confianza | Tickets | Story Points | Riesgo |
|-------------------|---------|--------------|--------|
| **Alto (>85%)** | US-001-01, US-001-03, US-001-10 | 13 pts (19%) | ‚úÖ Bajo riesgo |
| **Medio (70-85%)** | US-001-02, US-001-04, US-001-07, US-001-09 | 26 pts (38%) | ‚ö†Ô∏è Riesgo controlado |
| **Bajo (<70%)** | US-001-05, US-001-06, US-001-08 | 29 pts (43%) | üî¥ Alto riesgo |

**An√°lisis**: 43% de los story points est√°n en tickets de alta incertidumbre, principalmente AI/ML y scraping.

#### Critical Path Analysis

**Ruta Cr√≠tica** (no puede paralelizarse):
1. US-001-01 (DB Schema) ‚Üí 5 pts
2. US-001-02 (Upload API) ‚Üí 8 pts
3. US-001-04 (RabbitMQ) ‚Üí 5 pts
4. US-001-05 (Parser Service) ‚Üí 13 pts
5. US-001-03 (Status API) ‚Üí 5 pts
6. US-001-07 (Frontend UI) ‚Üí 8 pts
7. US-001-09 (E2E Tests) ‚Üí 5 pts

**Total Critical Path**: 49 pts (~3 semanas con 1 dev dedicado)

**Tickets Paralelizables**:
- US-001-06 (NLP Training): Puede desarrollarse en paralelo con US-001-07 (Frontend)
- US-001-08 (LinkedIn Scraper): Puede desarrollarse en paralelo con US-001-07
- US-001-10 (Docker Compose): Puede configurarse incrementalmente durante el desarrollo

#### Recomendaci√≥n de Sprint Planning

**Asumiendo:**
- Team de 3 devs full-stack (1 senior, 2 mid-level)
- Velocidad esperada: 40 pts / 2 semanas (sprint)
- Sprints de 2 semanas

**Sprint 1 (Semanas 1-2): Foundation**
- **Track 1 (Backend Dev 1)**: US-001-01 (5 pts) + US-001-02 (8 pts) = 13 pts
- **Track 2 (Backend Dev 2)**: US-001-05 (Parser Service) (13 pts) - En progreso
- **Track 3 (FullStack Dev 3)**: US-001-10 (3 pts) + US-001-04 (5 pts) = 8 pts
- **Total Sprint 1**: ~34 pts (capacidad: 40 pts) ‚úÖ

**Sprint 2 (Semanas 3-4): Integration & UX**
- **Track 1 (Backend Dev 1)**: US-001-03 (5 pts) + US-001-08 (LinkedIn Scraper) (8 pts) = 13 pts
- **Track 2 (Frontend Dev 2)**: US-001-07 (Frontend UI) (8 pts) + US-001-09 inicio = 10 pts
- **Track 3 (ML Dev 3)**: US-001-06 (NLP Training) (8 pts) + US-001-09 soporte = 10 pts
- **Total Sprint 2**: ~31 pts (capacidad: 40 pts) ‚úÖ

**Sprint 3 (Semana 5): Testing & Polish** (si es necesario)
- US-001-09 (E2E Tests completar) (5 pts)
- Bug fixes y polish (~8-10 pts)
- **Total Sprint 3**: ~15 pts

**Duraci√≥n Total Estimada**: **2-2.5 sprints (4-5 semanas)**

#### Cuellos de Botella Identificados

1. **US-001-05 (Parser Service)**: 13 pts, en critical path, alto riesgo
   - **Mitigaci√≥n**: Asignar a senior dev, considerar spike de 1 d√≠a, pair programming

2. **Dependencia de NLP accuracy**: US-001-05 y US-001-06 deben alcanzar >85% precisi√≥n
   - **Mitigaci√≥n**: Empezar con modelo base, US-001-06 es mejora iterativa (no blocker)

3. **LinkedIn Scraper (US-001-08)**: Alto riesgo legal/t√©cnico
   - **Mitigaci√≥n**: Marcar como opcional para MVP, priorizar PDF/DOCX upload primero

4. **Testing (US-001-09)**: Requiere todos los servicios funcionando
   - **Mitigaci√≥n**: Empezar tests unitarios e integraci√≥n desde Sprint 1, E2E al final

---

### An√°lisis de Riesgos de Estimaci√≥n

| Ticket | Riesgo | Probabilidad | Impacto | Estrategia de Mitigaci√≥n |
|--------|--------|--------------|---------|--------------------------|
| **US-001-05** | NLP accuracy <85% | Alta (60%) | Alto (+8 pts) | Spike 1 d√≠a, custom training (US-001-06), fallback a keyword matching |
| **US-001-05** | Performance >30s | Media (40%) | Alto (+5 pts) | Profiling, lazy loading, timeouts configurables, procesamiento incremental |
| **US-001-06** | Dataset insuficiente | Media (50%) | Medio (+3 pts) | Recolectar m√°s CVs, data augmentation, considerar BERT |
| **US-001-08** | LinkedIn blocking | Alta (70%) | Alto (+5 pts o eliminar) | Rate limiting agresivo, proxies, considerar eliminar de MVP |
| **US-001-02** | S3 upload failures | Media (30%) | Medio (+2 pts) | Retry con exponential backoff, circuit breaker, monitoring |
| **US-001-04** | Message loss | Baja (20%) | Alto (+3 pts) | Durable queues, manual ACK, tests exhaustivos |
| **US-001-09** | Flaky tests | Alta (60%) | Bajo (+2 pts) | Waiters apropiados, retries, isolation, debugging time |
| **US-001-07** | Responsive design issues | Media (40%) | Bajo (+2 pts) | Mobile-first, testing en dispositivos reales |

**Riesgo Total Agregado**: ~30 pts adicionales en worst case (44% overhead)

**Estimaci√≥n Conservadora**: 68 pts + 30 pts buffer = **~98 pts** (~2.5 sprints con team de 3 devs)

---

### Planning Poker Simulation

Simulaci√≥n de Planning Poker session con perspectivas de diferentes roles:

#### US-001-05 (Parser Service Core)

**Senior Backend Dev (Python expert)**:
- Estimaci√≥n: **8 pts**
- Justificaci√≥n: "He hecho parsers antes, spaCy es est√°ndar, pero la precisi√≥n >85% puede requerir iteraci√≥n. 8 pts es realista si uso bibliotecas probadas."

**Mid-Level FullStack Dev**:
- Estimaci√≥n: **13 pts**
- Justificaci√≥n: "No tengo experiencia con spaCy, m√∫ltiples parsers (PDF, DOCX), NLP, normalizaci√≥n, OCR fallback. Parece muy complejo, prefiero 13 pts."

**ML Engineer**:
- Estimaci√≥n: **13 pts**
- Justificaci√≥n: "La precisi√≥n de NLP es impredecible sin dataset de validaci√≥n. spaCy base models pueden no ser suficientes para CVs t√©cnicos. 13 pts incluye iteraciones."

**Consenso Final**: **13 pts** (mayor√≠a vote, considerando incertidumbre de ML)

---

#### US-001-08 (LinkedIn Scraper)

**Senior Backend Dev**:
- Estimaci√≥n: **13 pts**
- Justificaci√≥n: "LinkedIn tiene anti-bot muy agresivo, estructura HTML compleja, riesgo legal. Puede consumir mucho tiempo en debugging y workarounds."

**Mid-Level Dev**:
- Estimaci√≥n: **8 pts**
- Justificaci√≥n: "Si usamos playwright es relativamente simple, pero el anti-bot es un unknown. 8 pts si funciona, 13 si hay muchos blockers."

**Product Owner**:
- Opini√≥n: "¬øEs cr√≠tico para MVP? Podemos empezar solo con PDF/DOCX upload."

**Consenso Final**: **8 pts** (asumiendo implementaci√≥n) pero **marcar como OPCIONAL para MVP**

---

#### US-001-07 (Frontend UI)

**Senior Frontend Dev (React expert)**:
- Estimaci√≥n: **5 pts**
- Justificaci√≥n: "Drag-and-drop con react-dropzone es est√°ndar, polling con react-query es trivial, edici√≥n de campos es CRUD. 5 pts es suficiente."

**Mid-Level FullStack Dev**:
- Estimaci√≥n: **8 pts**
- Justificaci√≥n: "Hay muchos sub-componentes: upload, preview, edici√≥n, duplicados. Responsive design y accesibilidad agregan complejidad. Prefiero 8 pts."

**UX Designer**:
- Opini√≥n: "Necesitamos iteraci√≥n en UX, el flujo de edici√≥n debe ser muy intuitivo. Consideren tiempo para ajustes."

**Consenso Final**: **8 pts** (incluye iteraci√≥n de UX y tests de accesibilidad)

---

### Recomendaciones Finales

#### 1. Re-estimaci√≥n Triggers

Re-estimar tickets si:
- **US-001-05**: Despu√©s del spike de 1 d√≠a, si accuracy de spaCy base <70%
- **US-001-06**: Despu√©s de anotar primeros 20 CVs, si velocidad de anotaci√≥n >30min/CV
- **US-001-08**: Despu√©s de investigaci√≥n de API oficial (d√≠a 1), si no est√° disponible
- **US-001-09**: Despu√©s de setup de CI, si hay issues con service containers

#### 2. Estrategia de Mitigaci√≥n de Riesgos

**Alto Riesgo (US-001-05, US-001-06, US-001-08)**:
- ‚úÖ Asignar a developers senior o con experiencia relevante
- ‚úÖ Hacer spikes de 1 d√≠a antes de commitment
- ‚úÖ Pair programming en partes cr√≠ticas
- ‚úÖ Reviews t√©cnicos frecuentes (cada 2 d√≠as)
- ‚úÖ Tener Plan B definido (fallbacks, simplificaciones)

**Medio Riesgo (US-001-02, US-001-04, US-001-07)**:
- ‚úÖ Daily sync para identificar blockers temprano
- ‚úÖ Tests automatizados desde el inicio
- ‚úÖ Monitoring y logging robusto

**Bajo Riesgo (US-001-01, US-001-03, US-001-10)**:
- ‚úÖ Seguir process est√°ndar de desarrollo
- ‚úÖ Code reviews normales

#### 3. Optimizaciones de Scope para MVP

**Si hay presi√≥n de tiempo, considerar:**
- ‚ùå **Eliminar US-001-08** (LinkedIn Scraper): Riesgo alto, valor medio ‚Üí Backlog futuro
- ‚ùå **Eliminar US-001-06** (NLP Training): Opcional, usar spaCy base primero ‚Üí Mejora post-MVP
- ‚ö†Ô∏è **Simplificar US-001-05**: Omitir OCR fallback (AWS Textract) en MVP ‚Üí Agregar despu√©s
- ‚ö†Ô∏è **Simplificar US-001-07**: WebSocket real-time updates ‚Üí Usar solo polling

**Con optimizaciones**: 68 pts - 8 pts (US-001-08) - 8 pts (US-001-06) = **52 pts** (~1.5 sprints)

#### 4. M√©tricas de √âxito de Estimaciones

Trackear durante desarrollo:
- **Accuracy de estimaciones**: Story points reales vs estimados (target: ¬±20%)
- **Burn-down rate**: Completar ~20 pts/semana (team de 3 devs)
- **Tickets con re-work**: Target <10% de tickets requieren re-apertura
- **Velocity del team**: Medir en primeros 2 sprints para calibrar estimaciones futuras

---

## üìö Referencias y Recursos

### Documentaci√≥n T√©cnica
- [spaCy NER Documentation](https://spacy.io/usage/linguistic-features#named-entities)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [React 19 Documentation](https://react.dev/)
- [RabbitMQ Tutorials](https://www.rabbitmq.com/getstarted.html)
- [AWS Textract API Reference](https://docs.aws.amazon.com/textract/)

### Bibliotecas Clave
- **Python**: spaCy, FastAPI, PyPDF2, pdfplumber, python-docx, boto3, chardet
- **Node.js**: Express, TypeORM/Knex, multer, amqplib, @aws-sdk/client-s3
- **React**: react-dropzone, axios, react-query, react-hook-form

### Tools & Platforms
- **Development**: Docker, Docker Compose, VS Code, Postman
- **Testing**: Playwright, Jest, pytest, k6
- **CI/CD**: GitHub Actions (TBD)
- **Monitoring**: (TBD - considerar Datadog, New Relic, o Prometheus)

---

**Documento generado:** 2025-01-09
**Versi√≥n:** 1.0
**Pr√≥xima Revisi√≥n:** Despu√©s de Sprint 1 retrospective
